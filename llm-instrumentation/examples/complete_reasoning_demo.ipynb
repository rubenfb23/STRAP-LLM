{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Instrumentation Framework - Complete Demo with Reasoning Model\n",
    "\n",
    "This notebook demonstrates the complete capabilities of the **LLM Instrumentation Framework** using a small reasoning model. We'll capture activations separately during:\n",
    "1. **Prompt processing** (input phase)\n",
    "2. **Reasoning generation** (output phase)\n",
    "\n",
    "## What You'll Learn\n",
    "- How to configure and initialize the instrumentation framework\n",
    "- How to instrument a transformer model\n",
    "- How to capture activations at different granularities\n",
    "- How to analyze captured activation data\n",
    "- How to work with different compression algorithms\n",
    "- How to separate prompt vs. generation activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install the required dependencies and import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers torch accelerate\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Import the instrumentation framework\n",
    "from llm_instrumentation import (\n",
    "    InstrumentationFramework,\n",
    "    InstrumentationConfig,\n",
    "    HookGranularity,\n",
    ")\n",
    "\n",
    "print(\"‚úì All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load a Small Reasoning Model\n",
    "\n",
    "We'll use **Qwen2.5-0.5B-Instruct**, a small but capable reasoning model that's perfect for testing. It's only 500M parameters, so it runs quickly even on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take a minute...\\n\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"‚úì Tokenizer loaded\")\n",
    "\n",
    "# Load model (using CPU for compatibility, but GPU is supported)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,  # Use float32 for CPU compatibility\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(\"‚úì Model loaded\")\n",
    "\n",
    "# Print model architecture overview\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(f\"  - Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  - Layers: {len(list(model.named_modules()))}\")\n",
    "print(f\"  - Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure the Instrumentation Framework\n",
    "\n",
    "The framework supports multiple configurations:\n",
    "- **Granularity**: What to capture (full tensors, attention only, MLP only, sampled slices)\n",
    "- **Compression**: Algorithm to use (lz4, zstd, none)\n",
    "- **Throughput**: Target streaming rate\n",
    "- **Memory**: Maximum host RAM usage\n",
    "\n",
    "We'll create multiple configurations to demonstrate different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 1: Full tensor capture with LZ4 compression\n",
    "config_full = InstrumentationConfig(\n",
    "    granularity=HookGranularity.FULL_TENSOR,\n",
    "    compression_algorithm=\"lz4\",  # Fast compression\n",
    "    target_throughput_gbps=2.0,\n",
    "    max_memory_gb=8,\n",
    ")\n",
    "\n",
    "# Configuration 2: Attention-only capture (for interpretability)\n",
    "config_attention = InstrumentationConfig(\n",
    "    granularity=HookGranularity.ATTENTION_ONLY,\n",
    "    compression_algorithm=\"zstd\",  # Better compression ratio\n",
    "    target_throughput_gbps=2.0,\n",
    "    max_memory_gb=8,\n",
    ")\n",
    "\n",
    "# Configuration 3: MLP-only capture\n",
    "config_mlp = InstrumentationConfig(\n",
    "    granularity=HookGranularity.MLP_ONLY,\n",
    "    compression_algorithm=\"none\",  # No compression for speed\n",
    "    target_throughput_gbps=2.0,\n",
    "    max_memory_gb=8,\n",
    ")\n",
    "\n",
    "print(\"‚úì Configurations created:\")\n",
    "print(f\"  1. Full tensor capture with LZ4\")\n",
    "print(f\"  2. Attention-only with Zstd\")\n",
    "print(f\"  3. MLP-only with no compression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Reasoning Prompts\n",
    "\n",
    "We'll use prompts that require step-by-step reasoning to demonstrate how the model processes information differently during thinking vs. output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reasoning prompts\n",
    "prompts = [\n",
    "    \"\"\"Solve this step by step:\n",
    "If a train travels 120 km in 2 hours, what is its average speed?\n",
    "Show your reasoning.\"\"\",\n",
    "    \n",
    "    \"\"\"Think carefully:\n",
    "What is 15% of 200? Explain each step.\"\"\",\n",
    "    \n",
    "    \"\"\"Reason through this:\n",
    "If today is Wednesday, what day will it be in 10 days?\"\"\",\n",
    "]\n",
    "\n",
    "print(\"Reasoning prompts prepared:\")\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"\\n{i}. {prompt[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demo 1: Capture Prompt Processing Activations\n",
    "\n",
    "In this demo, we'll capture activations **only during the prompt processing phase**. We'll use the attention-only configuration to focus on how the model attends to different parts of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize framework for prompt capture\n",
    "framework_prompt = InstrumentationFramework(config_attention)\n",
    "\n",
    "# Register the model for instrumentation\n",
    "framework_prompt.instrument_model(model)\n",
    "print(\"‚úì Model instrumented for prompt capture\")\n",
    "\n",
    "# Select first prompt\n",
    "test_prompt = prompts[0]\n",
    "print(f\"\\nPrompt to process:\\n{test_prompt}\")\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "\n",
    "print(f\"\\nTokenized prompt:\")\n",
    "print(f\"  - Input shape: {input_ids.shape}\")\n",
    "print(f\"  - Number of tokens: {input_ids.shape[1]}\")\n",
    "print(f\"  - Device: {input_ids.device}\")\n",
    "\n",
    "# Capture activations during prompt processing\n",
    "output_path_prompt = \"prompt_activations.stream\"\n",
    "\n",
    "print(f\"\\nüîç Capturing activations during PROMPT PROCESSING...\")\n",
    "print(f\"   Output file: {output_path_prompt}\")\n",
    "print(f\"   Granularity: ATTENTION_ONLY\")\n",
    "print(f\"   Compression: zstd\\n\")\n",
    "\n",
    "with framework_prompt.capture_activations(output_path_prompt):\n",
    "    # Only forward pass (no generation)\n",
    "    with torch.no_grad():\n",
    "        outputs_prompt = model(input_ids)\n",
    "\n",
    "print(\"‚úì Prompt activations captured successfully!\")\n",
    "print(f\"  - Output shape: {outputs_prompt.logits.shape}\")\n",
    "print(f\"  - File created: {os.path.exists(output_path_prompt)}\")\n",
    "print(f\"  - File size: {os.path.getsize(output_path_prompt):,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Demo 2: Capture Generation (Reasoning) Activations\n",
    "\n",
    "Now we'll capture activations **during the generation phase**, when the model is actively reasoning and producing output tokens. This uses full tensor capture to get complete information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize framework for generation capture\n",
    "framework_generation = InstrumentationFramework(config_full)\n",
    "\n",
    "# Register the model\n",
    "framework_generation.instrument_model(model)\n",
    "print(\"‚úì Model instrumented for generation capture\")\n",
    "\n",
    "# Prepare the same prompt\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "\n",
    "# Capture activations during generation\n",
    "output_path_generation = \"generation_activations.stream\"\n",
    "\n",
    "print(f\"\\nüîç Capturing activations during GENERATION (Reasoning)...\")\n",
    "print(f\"   Output file: {output_path_generation}\")\n",
    "print(f\"   Granularity: FULL_TENSOR\")\n",
    "print(f\"   Compression: lz4\")\n",
    "print(f\"   Max new tokens: 100\\n\")\n",
    "\n",
    "with framework_generation.capture_activations(output_path_generation):\n",
    "    # Generate tokens (reasoning phase)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"‚úì Generation activations captured successfully!\")\n",
    "print(f\"  - File created: {os.path.exists(output_path_generation)}\")\n",
    "print(f\"  - File size: {os.path.getsize(output_path_generation):,} bytes\")\n",
    "print(f\"\\nüìù Generated response:\\n\")\n",
    "print(f\"{generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Captured Activations\n",
    "\n",
    "Now let's analyze both activation streams to see the differences between prompt processing and generation phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prompt activations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS: PROMPT PROCESSING ACTIVATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "analysis_prompt = framework_prompt.analyze_activations(output_path_prompt)\n",
    "\n",
    "print(f\"\\nüìä Overall Statistics:\")\n",
    "print(f\"  - Stream path: {analysis_prompt['stream_path']}\")\n",
    "print(f\"  - Compression: {analysis_prompt['compression']}\")\n",
    "print(f\"  - Total packets: {analysis_prompt['packets']:,}\")\n",
    "print(f\"  - Total compressed bytes: {analysis_prompt['total_compressed_bytes']:,}\")\n",
    "\n",
    "print(f\"\\nüìã Per-Layer Breakdown:\")\n",
    "for layer_name, stats in sorted(analysis_prompt['per_layer'].items()):\n",
    "    if layer_name:  # Skip empty names\n",
    "        avg_size = stats['bytes'] / stats['count'] if stats['count'] > 0 else 0\n",
    "        print(f\"  {layer_name:30s} ‚Üí {stats['count']:3d} packets, {stats['bytes']:10,} bytes (avg: {avg_size:,.0f} bytes/packet)\")\n",
    "\n",
    "# Analyze generation activations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS: GENERATION (REASONING) ACTIVATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "analysis_generation = framework_generation.analyze_activations(output_path_generation)\n",
    "\n",
    "print(f\"\\nüìä Overall Statistics:\")\n",
    "print(f\"  - Stream path: {analysis_generation['stream_path']}\")\n",
    "print(f\"  - Compression: {analysis_generation['compression']}\")\n",
    "print(f\"  - Total packets: {analysis_generation['packets']:,}\")\n",
    "print(f\"  - Total compressed bytes: {analysis_generation['total_compressed_bytes']:,}\")\n",
    "\n",
    "print(f\"\\nüìã Per-Layer Breakdown (top 10 by packet count):\")\n",
    "sorted_layers = sorted(analysis_generation['per_layer'].items(), \n",
    "                      key=lambda x: x[1]['count'], \n",
    "                      reverse=True)\n",
    "for layer_name, stats in sorted_layers[:10]:\n",
    "    if layer_name:\n",
    "        avg_size = stats['bytes'] / stats['count'] if stats['count'] > 0 else 0\n",
    "        print(f\"  {layer_name:30s} ‚Üí {stats['count']:3d} packets, {stats['bytes']:10,} bytes (avg: {avg_size:,.0f} bytes/packet)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Prompt vs. Generation Phases\n",
    "\n",
    "Let's create a comparison to understand how the model behaves differently during prompt processing vs. generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: PROMPT vs GENERATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison = {\n",
    "    \"Metric\": [\n",
    "        \"Total Packets\",\n",
    "        \"Total Compressed Bytes\",\n",
    "        \"Unique Layers\",\n",
    "        \"Compression Algorithm\",\n",
    "        \"Granularity\",\n",
    "    ],\n",
    "    \"Prompt Processing\": [\n",
    "        f\"{analysis_prompt['packets']:,}\",\n",
    "        f\"{analysis_prompt['total_compressed_bytes']:,}\",\n",
    "        f\"{len(analysis_prompt['per_layer'])}\",\n",
    "        analysis_prompt['compression'],\n",
    "        \"ATTENTION_ONLY\",\n",
    "    ],\n",
    "    \"Generation (Reasoning)\": [\n",
    "        f\"{analysis_generation['packets']:,}\",\n",
    "        f\"{analysis_generation['total_compressed_bytes']:,}\",\n",
    "        f\"{len(analysis_generation['per_layer'])}\",\n",
    "        analysis_generation['compression'],\n",
    "        \"FULL_TENSOR\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Print comparison table\n",
    "print(f\"\\n{'Metric':<30} {'Prompt Processing':<30} {'Generation'}\")\n",
    "print(\"-\" * 80)\n",
    "for i, metric in enumerate(comparison[\"Metric\"]):\n",
    "    print(f\"{metric:<30} {comparison['Prompt Processing'][i]:<30} {comparison['Generation (Reasoning)'][i]}\")\n",
    "\n",
    "# Calculate ratio\n",
    "if analysis_prompt['packets'] > 0:\n",
    "    packet_ratio = analysis_generation['packets'] / analysis_prompt['packets']\n",
    "    byte_ratio = analysis_generation['total_compressed_bytes'] / analysis_prompt['total_compressed_bytes']\n",
    "    \n",
    "    print(f\"\\nüìà Generation/Prompt Ratios:\")\n",
    "    print(f\"  - Packet ratio: {packet_ratio:.2f}x more packets during generation\")\n",
    "    print(f\"  - Data ratio: {byte_ratio:.2f}x more data during generation\")\n",
    "    print(f\"\\nüí° Interpretation:\")\n",
    "    print(f\"  Generation captures {packet_ratio:.1f}x more activations because:\")\n",
    "    print(f\"  1. Multiple forward passes (one per generated token)\")\n",
    "    print(f\"  2. Full tensor capture vs attention-only\")\n",
    "    print(f\"  3. The model processes {generated_ids.shape[1] - input_ids.shape[1]} new tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Demo 3: MLP-Only Capture for Feature Analysis\n",
    "\n",
    "Let's demonstrate MLP-only capture, which is useful for understanding how the model transforms representations in the feedforward layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize framework for MLP capture\n",
    "framework_mlp = InstrumentationFramework(config_mlp)\n",
    "framework_mlp.instrument_model(model)\n",
    "\n",
    "print(\"‚úì Model instrumented for MLP-only capture\")\n",
    "\n",
    "# Use a different prompt\n",
    "mlp_prompt = prompts[1]\n",
    "print(f\"\\nPrompt: {mlp_prompt}\\n\")\n",
    "\n",
    "inputs = tokenizer(mlp_prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "\n",
    "output_path_mlp = \"mlp_activations.stream\"\n",
    "\n",
    "print(f\"üîç Capturing MLP activations...\")\n",
    "print(f\"   Granularity: MLP_ONLY\")\n",
    "print(f\"   Compression: none\\n\")\n",
    "\n",
    "with framework_mlp.capture_activations(output_path_mlp):\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"‚úì MLP activations captured!\")\n",
    "print(f\"  - File size: {os.path.getsize(output_path_mlp):,} bytes\")\n",
    "print(f\"\\nüìù Generated response:\\n{generated_text}\")\n",
    "\n",
    "# Analyze MLP activations\n",
    "analysis_mlp = framework_mlp.analyze_activations(output_path_mlp)\n",
    "\n",
    "print(f\"\\nüìä MLP Analysis:\")\n",
    "print(f\"  - Total packets: {analysis_mlp['packets']:,}\")\n",
    "print(f\"  - Total bytes (uncompressed): {analysis_mlp['total_compressed_bytes']:,}\")\n",
    "print(f\"  - Unique MLP layers captured: {len(analysis_mlp['per_layer'])}\")\n",
    "\n",
    "if analysis_mlp['per_layer']:\n",
    "    print(f\"\\nüìã MLP Layers:\")\n",
    "    for layer_name in sorted(analysis_mlp['per_layer'].keys())[:5]:\n",
    "        if layer_name:\n",
    "            print(f\"  - {layer_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verification: Compare Compression Algorithms\n",
    "\n",
    "Let's verify compression effectiveness by comparing file sizes across different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all compression algorithms with the same prompt and settings\n",
    "compression_results = {}\n",
    "test_prompt_short = \"What is 2 + 2? Explain.\"\n",
    "inputs_test = tokenizer(test_prompt_short, return_tensors=\"pt\")\n",
    "input_ids_test = inputs_test[\"input_ids\"].to(model.device)\n",
    "\n",
    "print(\"\\nüß™ Testing compression algorithms...\\n\")\n",
    "\n",
    "for algo in [\"none\", \"lz4\", \"zstd\"]:\n",
    "    config_test = InstrumentationConfig(\n",
    "        granularity=HookGranularity.FULL_TENSOR,\n",
    "        compression_algorithm=algo,\n",
    "        target_throughput_gbps=2.0,\n",
    "        max_memory_gb=8,\n",
    "    )\n",
    "    \n",
    "    framework_test = InstrumentationFramework(config_test)\n",
    "    framework_test.instrument_model(model)\n",
    "    \n",
    "    output_path_test = f\"compression_test_{algo}.stream\"\n",
    "    \n",
    "    print(f\"Testing {algo:6s} compression...\", end=\" \")\n",
    "    \n",
    "    with framework_test.capture_activations(output_path_test):\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(\n",
    "                input_ids_test,\n",
    "                max_new_tokens=30,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "    \n",
    "    file_size = os.path.getsize(output_path_test)\n",
    "    compression_results[algo] = file_size\n",
    "    print(f\"‚úì {file_size:,} bytes\")\n",
    "\n",
    "# Calculate compression ratios\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPRESSION COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "baseline = compression_results[\"none\"]\n",
    "print(f\"\\n{'Algorithm':<10} {'Size (bytes)':<15} {'Compression Ratio':<20} {'Reduction'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for algo, size in compression_results.items():\n",
    "    ratio = baseline / size if size > 0 else 0\n",
    "    reduction = (1 - size/baseline) * 100 if baseline > 0 else 0\n",
    "    print(f\"{algo:<10} {size:<15,} {ratio:<20.2f}x {reduction:>5.1f}% reduction\")\n",
    "\n",
    "print(f\"\\nüí° Best compression: {min(compression_results, key=compression_results.get)}\")\n",
    "print(f\"üí° Fastest (no compression): none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Verification\n",
    "\n",
    "Let's verify that everything worked correctly and summarize what we've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VERIFICATION CHECKLIST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "checks = [\n",
    "    (\"Model loaded successfully\", model is not None),\n",
    "    (\"Prompt activations captured\", os.path.exists(output_path_prompt)),\n",
    "    (\"Generation activations captured\", os.path.exists(output_path_generation)),\n",
    "    (\"MLP activations captured\", os.path.exists(output_path_mlp)),\n",
    "    (\"Prompt analysis completed\", analysis_prompt['packets'] > 0),\n",
    "    (\"Generation analysis completed\", analysis_generation['packets'] > 0),\n",
    "    (\"MLP analysis completed\", analysis_mlp['packets'] > 0),\n",
    "    (\"Compression test completed\", len(compression_results) == 3),\n",
    "    (\"Model generated valid output\", len(generated_text) > len(test_prompt)),\n",
    "]\n",
    "\n",
    "all_passed = True\n",
    "for check_name, result in checks:\n",
    "    status = \"‚úì\" if result else \"‚úó\"\n",
    "    print(f\"{status} {check_name}\")\n",
    "    if not result:\n",
    "        all_passed = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if all_passed:\n",
    "    print(\"‚úì ALL CHECKS PASSED - Framework is working correctly!\")\n",
    "else:\n",
    "    print(\"‚úó Some checks failed - please review the output above\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: What You've Learned\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "‚úì How to configure the instrumentation framework with different settings\n",
    "‚úì How to instrument a transformer model for activation capture\n",
    "‚úì How to capture activations separately for:\n",
    "  - Prompt processing (input phase)\n",
    "  - Generation/reasoning (output phase)\n",
    "‚úì How to use different granularities:\n",
    "  - FULL_TENSOR: Complete activation capture\n",
    "  - ATTENTION_ONLY: Focus on attention mechanisms\n",
    "  - MLP_ONLY: Focus on feedforward layers\n",
    "‚úì How to compare compression algorithms:\n",
    "  - none: Fastest, largest files\n",
    "  - lz4: Fast compression, good ratio\n",
    "  - zstd: Best compression, slower\n",
    "‚úì How to analyze captured activation streams\n",
    "‚úì How to interpret per-layer statistics\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FILES GENERATED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "1. {output_path_prompt} ({os.path.getsize(output_path_prompt):,} bytes)\n",
    "   ‚Üí Attention-only activations from prompt processing\n",
    "   \n",
    "2. {output_path_generation} ({os.path.getsize(output_path_generation):,} bytes)\n",
    "   ‚Üí Full tensor activations from reasoning/generation\n",
    "   \n",
    "3. {output_path_mlp} ({os.path.getsize(output_path_mlp):,} bytes)\n",
    "   ‚Üí MLP-only activations (uncompressed)\n",
    "   \n",
    "4. compression_test_*.stream (3 files)\n",
    "   ‚Üí Compression algorithm comparison\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "Now you can:\n",
    "1. Use these activation streams for downstream analysis\n",
    "2. Feed them into interpretability tools (SAE, causal graphs)\n",
    "3. Correlate activations with model behavior\n",
    "4. Optimize model performance based on activation patterns\n",
    "5. Compare activations across different prompts/models\n",
    "\n",
    "For more information, see the documentation:\n",
    "- docs/API.md - Complete API reference\n",
    "- docs/STREAM_FORMAT.md - Stream format specification\n",
    "- docs/ARCHITECTURE.md - Framework architecture\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cleanup (Optional)\n",
    "\n",
    "Uncomment and run this cell to remove generated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# files_to_remove = [\n",
    "#     output_path_prompt,\n",
    "#     output_path_generation,\n",
    "#     output_path_mlp,\n",
    "#     \"compression_test_none.stream\",\n",
    "#     \"compression_test_lz4.stream\",\n",
    "#     \"compression_test_zstd.stream\",\n",
    "# ]\n",
    "\n",
    "# for file_path in files_to_remove:\n",
    "#     if os.path.exists(file_path):\n",
    "#         os.remove(file_path)\n",
    "#         print(f\"Removed: {file_path}\")\n",
    "\n",
    "# print(\"\\n‚úì Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
