{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Instrumentation Framework - Complete Demo with Reasoning Model\n",
    "\n",
    "This notebook demonstrates the complete capabilities of the **LLM Instrumentation Framework** using a small reasoning model. We'll capture activations separately during:\n",
    "1. **Prompt processing** (input phase)\n",
    "2. **Reasoning generation** (output phase)\n",
    "\n",
    "## What You'll Learn\n",
    "- How to configure and initialize the instrumentation framework\n",
    "- How to instrument a transformer model\n",
    "- How to capture activations at different granularities\n",
    "- How to analyze captured activation data\n",
    "- How to work with different compression algorithms\n",
    "- How to separate prompt vs. generation activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install the required dependencies and import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 All imports successful!\n",
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers torch accelerate\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import json\n",
    "import llm_instrumentation\n",
    "\n",
    "# Import the instrumentation framework\n",
    "from llm_instrumentation import (\n",
    "    InstrumentationFramework,\n",
    "    InstrumentationConfig,\n",
    "    HookGranularity,\n",
    ")\n",
    "\n",
    "print(\"\u2713 All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load a Small Reasoning Model\n",
    "\n",
    "We'll use **Qwen2.5-0.5B-Instruct**, a small but capable reasoning model that's perfect for testing. It's only 500M parameters, so it runs quickly even on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "This may take a minute...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Tokenizer loaded\n",
      "\u2713 Model loaded\n",
      "\n",
      "Model architecture:\n",
      "  - Total parameters: 494,032,768\n",
      "  - Layers: 319\n",
      "  - Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take a minute...\\n\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"\u2713 Tokenizer loaded\")\n",
    "\n",
    "# Load model (using CPU for compatibility, but GPU is supported)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,  # Use float32 for CPU compatibility\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(\"\u2713 Model loaded\")\n",
    "\n",
    "# Print model architecture overview\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(f\"  - Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  - Layers: {len(list(model.named_modules()))}\")\n",
    "print(f\"  - Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure the Instrumentation Framework\n",
    "\n",
    "The framework supports multiple configurations:\n",
    "- **Granularity**: What to capture (full tensors, attention only, MLP only, sampled slices)\n",
    "- **Compression**: Algorithm to use (lz4, zstd, none)\n",
    "- **Throughput**: Target streaming rate\n",
    "- **Memory**: Maximum host RAM usage\n",
    "\n",
    "We'll create multiple configurations to demonstrate different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Configurations created:\n",
      "  1. Full tensor capture with LZ4\n",
      "  2. Attention-only with Zstd\n",
      "  3. MLP-only with no compression\n"
     ]
    }
   ],
   "source": [
    "# Configuration 1: Full tensor capture with LZ4 compression\n",
    "config_full = InstrumentationConfig(\n",
    "    granularity=HookGranularity.FULL_TENSOR,\n",
    "    compression_algorithm=\"lz4\",  # Fast compression\n",
    "    target_throughput_gbps=2.0,\n",
    "    max_memory_gb=8,\n",
    ")\n",
    "\n",
    "# Configuration 2: Attention-only capture (for interpretability)\n",
    "config_attention = InstrumentationConfig(\n",
    "    granularity=HookGranularity.ATTENTION_ONLY,\n",
    "    compression_algorithm=\"zstd\",  # Better compression ratio\n",
    "    target_throughput_gbps=2.0,\n",
    "    max_memory_gb=8,\n",
    ")\n",
    "\n",
    "# Configuration 3: MLP-only capture\n",
    "config_mlp = InstrumentationConfig(\n",
    "    granularity=HookGranularity.MLP_ONLY,\n",
    "    compression_algorithm=\"none\",  # No compression for speed\n",
    "    target_throughput_gbps=2.0,\n",
    "    max_memory_gb=8,\n",
    ")\n",
    "\n",
    "print(\"\u2713 Configurations created:\")\n",
    "print(f\"  1. Full tensor capture with LZ4\")\n",
    "print(f\"  2. Attention-only with Zstd\")\n",
    "print(f\"  3. MLP-only with no compression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Reasoning Prompts\n",
    "\n",
    "We'll use prompts that require step-by-step reasoning to demonstrate how the model processes information differently during thinking vs. output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning prompts prepared:\n",
      "\n",
      "1. Solve this step by step:\n",
      "If a train travels 120 km...\n",
      "\n",
      "2. Think carefully:\n",
      "What is 15% of 200? Explain each ...\n",
      "\n",
      "3. Reason through this:\n",
      "If today is Wednesday, what d...\n"
     ]
    }
   ],
   "source": [
    "# Define reasoning prompts\n",
    "prompts = [\n",
    "    \"\"\"Solve this step by step:\n",
    "If a train travels 120 km in 2 hours, what is its average speed?\n",
    "Show your reasoning.\"\"\",\n",
    "    \n",
    "    \"\"\"Think carefully:\n",
    "What is 15% of 200? Explain each step.\"\"\",\n",
    "    \n",
    "    \"\"\"Reason through this:\n",
    "If today is Wednesday, what day will it be in 10 days?\"\"\",\n",
    "]\n",
    "\n",
    "print(\"Reasoning prompts prepared:\")\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"\\n{i}. {prompt[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demo 1: Capture Prompt Processing Activations\n",
    "\n",
    "In this demo, we'll capture activations **only during the prompt processing phase**. We'll use the attention-only configuration to focus on how the model attends to different parts of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Model instrumented for prompt capture\n",
      "\n",
      "Prompt to process:\n",
      "Solve this step by step:\n",
      "If a train travels 120 km in 2 hours, what is its average speed?\n",
      "Show your reasoning.\n",
      "\n",
      "Tokenized prompt:\n",
      "  - Input shape: torch.Size([1, 31])\n",
      "  - Number of tokens: 31\n",
      "  - Device: cuda:0\n",
      "\n",
      "\ud83d\udd0d Capturing activations during PROMPT PROCESSING...\n",
      "   Output file: prompt_activations.stream\n",
      "   Granularity: ATTENTION_ONLY\n",
      "   Compression: zstd\n",
      "\n",
      "\u2713 Prompt activations captured successfully!\n",
      "  - Output shape: torch.Size([1, 31, 151936])\n",
      "  - File created: True\n",
      "  - File size: 0 bytes\n"
     ]
    }
   ],
   "source": [
    "# Initialize framework for prompt capture\n",
    "framework_prompt = InstrumentationFramework(config_attention)\n",
    "\n",
    "# Register the model for instrumentation\n",
    "framework_prompt.instrument_model(model)\n",
    "print(\"\u2713 Model instrumented for prompt capture\")\n",
    "\n",
    "# Select first prompt\n",
    "test_prompt = prompts[0]\n",
    "print(f\"\\nPrompt to process:\\n{test_prompt}\")\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "\n",
    "print(f\"\\nTokenized prompt:\")\n",
    "print(f\"  - Input shape: {input_ids.shape}\")\n",
    "print(f\"  - Number of tokens: {input_ids.shape[1]}\")\n",
    "print(f\"  - Device: {input_ids.device}\")\n",
    "\n",
    "# Capture activations during prompt processing\n",
    "output_path_prompt = \"prompt_activations.stream\"\n",
    "\n",
    "print(f\"\\n\ud83d\udd0d Capturing activations during PROMPT PROCESSING...\")\n",
    "print(f\"   Output file: {output_path_prompt}\")\n",
    "print(f\"   Granularity: ATTENTION_ONLY\")\n",
    "print(f\"   Compression: zstd\\n\")\n",
    "\n",
    "with framework_prompt.capture_activations(output_path_prompt):\n",
    "    # Only forward pass (no generation)\n",
    "    with torch.no_grad():\n",
    "        outputs_prompt = model(input_ids)\n",
    "\n",
    "print(\"\u2713 Prompt activations captured successfully!\")\n",
    "print(f\"  - Output shape: {outputs_prompt.logits.shape}\")\n",
    "print(f\"  - File created: {os.path.exists(output_path_prompt)}\")\n",
    "print(f\"  - File size: {os.path.getsize(output_path_prompt):,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Demo 2: Capture Generation (Reasoning) Activations\n",
    "\n",
    "Now we'll capture activations **during the generation phase**, when the model is actively reasoning and producing output tokens. This uses full tensor capture to get complete information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Model instrumented for generation capture\n",
      "\n",
      "\ud83d\udd0d Capturing activations during GENERATION (Reasoning)...\n",
      "   Output file: generation_activations.stream\n",
      "   Granularity: FULL_TENSOR\n",
      "   Compression: lz4\n",
      "   Max new tokens: 512\n",
      "\n",
      "\u2713 Generation activations captured successfully!\n",
      "  - File created: True\n",
      "  - File size: 64,953,573 bytes\n",
      "\n",
      "\ud83d\udcdd Generated response:\n",
      "\n",
      "Solve this step by step:\n",
      "If a train travels 120 km in 2 hours, what is its average speed?\n",
      "Show your reasoning. Let's solve this problem step by step:\n",
      "\n",
      "Step 1: Identify the given information\n",
      "- Distance traveled (d) = 120 km\n",
      "- Time taken (t) = 2 hours\n",
      "\n",
      "Step 2: Recall the formula for average speed\n",
      "Average speed (v_avg) = d/t\n",
      "\n",
      "Step 3: Plug in the values into the formula\n",
      "v_avg = d/t = 120 km / 2 h\n",
      "\n",
      "Step 4: Simplify the equation\n",
      "v_avg = 60 km/h\n",
      "\n",
      "Therefore, the average speed of the train is 60 kilometers per hour.\n",
      "\n",
      "The reasoning process shows that we used the definition of average speed to find the result directly from the given distance and time. This type of calculation involves basic arithmetic operations and understanding of how to apply these operations to real-world scenarios. In this case, it was necessary to convert the units from kilometers to meters to match the time unit (hours). If you need more practice with conversions or other types of problems like this, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# Initialize framework for generation capture\n",
    "framework_generation = InstrumentationFramework(config_full)\n",
    "\n",
    "# Register the model\n",
    "framework_generation.instrument_model(model)\n",
    "print(\"\u2713 Model instrumented for generation capture\")\n",
    "\n",
    "# Prepare the same prompt\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "\n",
    "# Capture activations during generation\n",
    "output_path_generation = \"generation_activations.stream\"\n",
    "\n",
    "print(f\"\\n\ud83d\udd0d Capturing activations during GENERATION (Reasoning)...\")\n",
    "print(f\"   Output file: {output_path_generation}\")\n",
    "print(f\"   Granularity: FULL_TENSOR\")\n",
    "print(f\"   Compression: lz4\")\n",
    "print(f\"   Max new tokens: 1024\\n\")\n",
    "\n",
    "with framework_generation.capture_activations(output_path_generation):\n",
    "    # Generate tokens (reasoning phase)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\u2713 Generation activations captured successfully!\")\n",
    "print(f\"  - File created: {os.path.exists(output_path_generation)}\")\n",
    "print(f\"  - File size: {os.path.getsize(output_path_generation):,} bytes\")\n",
    "print(f\"\\n\ud83d\udcdd Generated response:\\n\")\n",
    "print(f\"{generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Demo 3: Per-Token Capture (Reasoning Analysis)\n\n",
    "Now let's demonstrate the **per-token tracking feature**. This is particularly useful for analyzing how activations change with each generated token during reasoning. We'll manually generate tokens one-by-one and track each one.\n\n",
    "The `track_per_token=True` flag enables per-token metadata collection, which is saved to a separate JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize framework for per-token capture\n",
    "framework_per_token = InstrumentationFramework(config_full)\n",
    "framework_per_token.instrument_model(model)\n",
    "\n",
    "print(\"\u2713 Model instrumented for per-token capture\")\n",
    "\n",
    "# Select a reasoning prompt\n",
    "test_prompt_token = prompts[0]\n",
    "print(f\"\\nPrompt: {test_prompt_token[:80]}...\\n\")\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(test_prompt_token, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "\n",
    "output_path_per_token = \"generation_per_token.stream\"\n",
    "\n",
    "print(f\"\ud83d\udd0d Capturing activations with per-token tracking...\")\n",
    "print(f\"   Output file: {output_path_per_token}\")\n",
    "print(f\"   Token metadata: {output_path_per_token.replace('.stream', '_tokens.json')}\")\n",
    "print(f\"   Max tokens: 100\\n\")\n",
    "\n",
    "# Capture with per-token tracking enabled\n",
    "with framework_per_token.capture_activations(output_path_per_token, track_per_token=True) as tracker:\n",
    "    current_ids = input_ids\n",
    "    generated_tokens = []\n",
    "    \n",
    "    for step in range(100):\n",
    "        # Forward pass to get next token\n",
    "        with torch.no_grad():\n",
    "            outputs = model(current_ids)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = next_token_logits.argmax(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Record the token with the tracker\n",
    "        token_id = next_token[0].item()\n",
    "        token_text = tokenizer.decode(next_token[0])\n",
    "        tracker.record_token(\n",
    "            token_id=token_id,\n",
    "            token_text=token_text\n",
    "        )\n",
    "        \n",
    "        generated_tokens.append(token_text)\n",
    "        \n",
    "        # Append to input sequence\n",
    "        current_ids = torch.cat([current_ids, next_token], dim=-1)\n",
    "        \n",
    "        # Stop if we hit EOS token\n",
    "        if token_id == tokenizer.eos_token_id:\n",
    "            print(f\"\u2713 Generation completed at token {step + 1} (EOS reached)\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"\u2713 Generation completed after {step + 1} tokens (max reached)\")\n",
    "\n",
    "# Decode the full generated sequence\n",
    "full_text = tokenizer.decode(current_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\n\ud83d\udcdd Generated {len(generated_tokens)} tokens\")\n",
    "print(f\"  - File created: {os.path.exists(output_path_per_token)}\")\n",
    "print(f\"  - File size: {os.path.getsize(output_path_per_token):,} bytes\")\n",
    "print(f\"\\n\ud83d\udcc4 Full generated text:\\n{full_text[:500]}...\")\n",
    "\n",
    "# Analyze with token metadata using the helper function\n",
    "from llm_instrumentation import analyze_activations_with_tokens\n",
    "\n",
    "analysis_per_token = analyze_activations_with_tokens(output_path_per_token, framework_per_token)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Per-Token Analysis:\")\n",
    "print(f\"  - Total tokens generated: {analysis_per_token.get('token_metadata', {}).get('total_tokens', 0)}\")\n",
    "print(f\"  - Total packets captured: {analysis_per_token['packets']:,}\")\n",
    "\n",
    "if 'packets_per_token' in analysis_per_token:\n",
    "    print(f\"  - Average packets per token: {analysis_per_token['packets_per_token']:.2f}\")\n",
    "    print(f\"  - Average bytes per token: {analysis_per_token['bytes_per_token']:,.0f}\")\n",
    "\n",
    "if 'token_metadata' in analysis_per_token:\n",
    "    print(f\"\\n\ud83d\udccb First 10 tokens generated:\")\n",
    "    for token_info in analysis_per_token['token_metadata']['tokens'][:10]:\n",
    "        print(f\"    [{token_info['token_index']}] Token ID {token_info['token_id']}: '{token_info['token_text']}'\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udca1 Use Case:\")\n",
    "    print(f\"  This per-token capture allows you to:\")\n",
    "    print(f\"  - Correlate activations with specific reasoning steps\")\n",
    "    print(f\"  - Analyze how model confidence evolves during generation\")\n",
    "    print(f\"  - Identify patterns in activation changes between tokens\")\n",
    "    print(f\"  - Debug specific points in the reasoning chain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Captured Activations\n",
    "\n",
    "Now let's analyze both activation streams to see the differences between prompt processing and generation phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANALYSIS: PROMPT PROCESSING ACTIVATIONS\n",
      "============================================================\n",
      "\n",
      "\ud83d\udcca Overall Statistics:\n",
      "  - Stream path: prompt_activations.stream\n",
      "  - Compression: zstd\n",
      "  - Total packets: 0\n",
      "  - Total compressed bytes: 0\n",
      "\n",
      "\ud83d\udccb Per-Layer Breakdown:\n",
      "\n",
      "============================================================\n",
      "ANALYSIS: GENERATION (REASONING) ACTIVATIONS\n",
      "============================================================\n",
      "\n",
      "\ud83d\udcca Overall Statistics:\n",
      "  - Stream path: generation_activations.stream\n",
      "  - Compression: lz4\n",
      "  - Total packets: 153\n",
      "  - Total compressed bytes: 46,436,630\n",
      "\n",
      "\ud83d\udccb Per-Layer Breakdown (top 10 by packet count):\n",
      "  lm_head                        \u2192 153 packets, 46,436,630 bytes (avg: 303,507 bytes/packet)\n"
     ]
    }
   ],
   "source": [
    "# Analyze prompt activations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS: PROMPT PROCESSING ACTIVATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "analysis_prompt = framework_prompt.analyze_activations(output_path_prompt)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Overall Statistics:\")\n",
    "print(f\"  - Stream path: {analysis_prompt['stream_path']}\")\n",
    "print(f\"  - Compression: {analysis_prompt['compression']}\")\n",
    "print(f\"  - Total packets: {analysis_prompt['packets']:,}\")\n",
    "print(f\"  - Total compressed bytes: {analysis_prompt['total_compressed_bytes']:,}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Per-Layer Breakdown:\")\n",
    "for layer_name, stats in sorted(analysis_prompt['per_layer'].items()):\n",
    "    if layer_name:  # Skip empty names\n",
    "        avg_size = stats['bytes'] / stats['count'] if stats['count'] > 0 else 0\n",
    "        print(f\"  {layer_name:30s} \u2192 {stats['count']:3d} packets, {stats['bytes']:10,} bytes (avg: {avg_size:,.0f} bytes/packet)\")\n",
    "\n",
    "# Analyze generation activations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS: GENERATION (REASONING) ACTIVATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "analysis_generation = framework_generation.analyze_activations(output_path_generation)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Overall Statistics:\")\n",
    "print(f\"  - Stream path: {analysis_generation['stream_path']}\")\n",
    "print(f\"  - Compression: {analysis_generation['compression']}\")\n",
    "print(f\"  - Total packets: {analysis_generation['packets']:,}\")\n",
    "print(f\"  - Total compressed bytes: {analysis_generation['total_compressed_bytes']:,}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Per-Layer Breakdown (top 10 by packet count):\")\n",
    "sorted_layers = sorted(analysis_generation['per_layer'].items(), \n",
    "                      key=lambda x: x[1]['count'], \n",
    "                      reverse=True)\n",
    "for layer_name, stats in sorted_layers[:10]:\n",
    "    if layer_name:\n",
    "        avg_size = stats['bytes'] / stats['count'] if stats['count'] > 0 else 0\n",
    "        print(f\"  {layer_name:30s} \u2192 {stats['count']:3d} packets, {stats['bytes']:10,} bytes (avg: {avg_size:,.0f} bytes/packet)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare Prompt vs. Generation Phases\n",
    "\n",
    "Let's create a comparison to understand how the model behaves differently during prompt processing vs. generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPARISON: PROMPT vs GENERATION\n",
      "============================================================\n",
      "\n",
      "Metric                         Prompt Processing              Generation\n",
      "--------------------------------------------------------------------------------\n",
      "Total Packets                  0                              153\n",
      "Total Compressed Bytes         0                              46,436,630\n",
      "Unique Layers                  0                              1\n",
      "Compression Algorithm          zstd                           lz4\n",
      "Granularity                    ATTENTION_ONLY                 FULL_TENSOR\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: PROMPT vs GENERATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison = {\n",
    "    \"Metric\": [\n",
    "        \"Total Packets\",\n",
    "        \"Total Compressed Bytes\",\n",
    "        \"Unique Layers\",\n",
    "        \"Compression Algorithm\",\n",
    "        \"Granularity\",\n",
    "    ],\n",
    "    \"Prompt Processing\": [\n",
    "        f\"{analysis_prompt['packets']:,}\",\n",
    "        f\"{analysis_prompt['total_compressed_bytes']:,}\",\n",
    "        f\"{len(analysis_prompt['per_layer'])}\",\n",
    "        analysis_prompt['compression'],\n",
    "        \"ATTENTION_ONLY\",\n",
    "    ],\n",
    "    \"Generation (Reasoning)\": [\n",
    "        f\"{analysis_generation['packets']:,}\",\n",
    "        f\"{analysis_generation['total_compressed_bytes']:,}\",\n",
    "        f\"{len(analysis_generation['per_layer'])}\",\n",
    "        analysis_generation['compression'],\n",
    "        \"FULL_TENSOR\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Print comparison table\n",
    "print(f\"\\n{'Metric':<30} {'Prompt Processing':<30} {'Generation'}\")\n",
    "print(\"-\" * 80)\n",
    "for i, metric in enumerate(comparison[\"Metric\"]):\n",
    "    print(f\"{metric:<30} {comparison['Prompt Processing'][i]:<30} {comparison['Generation (Reasoning)'][i]}\")\n",
    "\n",
    "# Calculate ratio\n",
    "if analysis_prompt['packets'] > 0:\n",
    "    packet_ratio = analysis_generation['packets'] / analysis_prompt['packets']\n",
    "    byte_ratio = analysis_generation['total_compressed_bytes'] / analysis_prompt['total_compressed_bytes']\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcc8 Generation/Prompt Ratios:\")\n",
    "    print(f\"  - Packet ratio: {packet_ratio:.2f}x more packets during generation\")\n",
    "    print(f\"  - Data ratio: {byte_ratio:.2f}x more data during generation\")\n",
    "    print(f\"\\n\ud83d\udca1 Interpretation:\")\n",
    "    print(f\"  Generation captures {packet_ratio:.1f}x more activations because:\")\n",
    "    print(f\"  1. Multiple forward passes (one per generated token)\")\n",
    "    print(f\"  2. Full tensor capture vs attention-only\")\n",
    "    print(f\"  3. The model processes {generated_ids.shape[1] - input_ids.shape[1]} new tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Demo 4: MLP-Only Capture for Feature Analysis\n",
    "\n",
    "Let's demonstrate MLP-only capture, which is useful for understanding how the model transforms representations in the feedforward layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Model instrumented for MLP-only capture\n",
      "\n",
      "Prompt: Think carefully:\n",
      "What is 15% of 200? Explain each step.\n",
      "\n",
      "\ud83d\udd0d Capturing MLP activations...\n",
      "   Granularity: MLP_ONLY\n",
      "   Compression: none\n",
      "\n",
      "\u2713 MLP activations captured!\n",
      "  - File size: 0 bytes\n",
      "\n",
      "\ud83d\udcdd Generated response:\n",
      "Think carefully:\n",
      "What is 15% of 200? Explain each step. To find what 15% of 200 is, follow these steps:\n",
      "\n",
      "1. **Understand the problem**: We need to calculate 15% of a number. In mathematical terms, this is represented as \\( 15\n",
      "\n",
      "\ud83d\udcca MLP Analysis:\n",
      "  - Total packets: 0\n",
      "  - Total bytes (uncompressed): 0\n",
      "  - Unique MLP layers captured: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize framework for MLP capture\n",
    "framework_mlp = InstrumentationFramework(config_mlp)\n",
    "framework_mlp.instrument_model(model)\n",
    "\n",
    "print(\"\u2713 Model instrumented for MLP-only capture\")\n",
    "\n",
    "# Use a different prompt\n",
    "mlp_prompt = prompts[1]\n",
    "print(f\"\\nPrompt: {mlp_prompt}\\n\")\n",
    "\n",
    "inputs = tokenizer(mlp_prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "\n",
    "output_path_mlp = \"mlp_activations.stream\"\n",
    "\n",
    "print(f\"\ud83d\udd0d Capturing MLP activations...\")\n",
    "print(f\"   Granularity: MLP_ONLY\")\n",
    "print(f\"   Compression: none\\n\")\n",
    "\n",
    "with framework_mlp.capture_activations(output_path_mlp):\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\u2713 MLP activations captured!\")\n",
    "print(f\"  - File size: {os.path.getsize(output_path_mlp):,} bytes\")\n",
    "print(f\"\\n\ud83d\udcdd Generated response:\\n{generated_text}\")\n",
    "\n",
    "# Analyze MLP activations\n",
    "analysis_mlp = framework_mlp.analyze_activations(output_path_mlp)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca MLP Analysis:\")\n",
    "print(f\"  - Total packets: {analysis_mlp['packets']:,}\")\n",
    "print(f\"  - Total bytes (uncompressed): {analysis_mlp['total_compressed_bytes']:,}\")\n",
    "print(f\"  - Unique MLP layers captured: {len(analysis_mlp['per_layer'])}\")\n",
    "\n",
    "if analysis_mlp['per_layer']:\n",
    "    print(f\"\\n\ud83d\udccb MLP Layers:\")\n",
    "    for layer_name in sorted(analysis_mlp['per_layer'].keys())[:5]:\n",
    "        if layer_name:\n",
    "            print(f\"  - {layer_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Verification: Compare Compression Algorithms\n",
    "\n",
    "Let's verify compression effectiveness by comparing file sizes across different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83e\uddea Testing compression algorithms...\n",
      "\n",
      "Testing none   compression... \u2713 9,116,550 bytes\n",
      "Testing lz4    compression... \u2713 9,106,360 bytes\n",
      "Testing zstd   compression... \u2713 8,121,115 bytes\n",
      "\n",
      "============================================================\n",
      "COMPRESSION COMPARISON\n",
      "============================================================\n",
      "\n",
      "Algorithm  Size (bytes)    Compression Ratio    Reduction\n",
      "----------------------------------------------------------------------\n",
      "none       9,116,550       1.00                x   0.0% reduction\n",
      "lz4        9,106,360       1.00                x   0.1% reduction\n",
      "zstd       8,121,115       1.12                x  10.9% reduction\n",
      "\n",
      "\ud83d\udca1 Best compression: zstd\n",
      "\ud83d\udca1 Fastest (no compression): none\n"
     ]
    }
   ],
   "source": [
    "# Test all compression algorithms with the same prompt and settings\n",
    "compression_results = {}\n",
    "test_prompt_short = \"What is 2 + 2? Explain.\"\n",
    "inputs_test = tokenizer(test_prompt_short, return_tensors=\"pt\")\n",
    "input_ids_test = inputs_test[\"input_ids\"].to(model.device)\n",
    "\n",
    "print(\"\\n\ud83e\uddea Testing compression algorithms...\\n\")\n",
    "\n",
    "for algo in [\"none\", \"lz4\", \"zstd\"]:\n",
    "    config_test = InstrumentationConfig(\n",
    "        granularity=HookGranularity.FULL_TENSOR,\n",
    "        compression_algorithm=algo,\n",
    "        target_throughput_gbps=2.0,\n",
    "        max_memory_gb=8,\n",
    "    )\n",
    "    \n",
    "    framework_test = InstrumentationFramework(config_test)\n",
    "    framework_test.instrument_model(model)\n",
    "    \n",
    "    output_path_test = f\"compression_test_{algo}.stream\"\n",
    "    \n",
    "    print(f\"Testing {algo:6s} compression...\", end=\" \")\n",
    "    \n",
    "    with framework_test.capture_activations(output_path_test):\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(\n",
    "                input_ids_test,\n",
    "                max_new_tokens=30,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "    \n",
    "    file_size = os.path.getsize(output_path_test)\n",
    "    compression_results[algo] = file_size\n",
    "    print(f\"\u2713 {file_size:,} bytes\")\n",
    "\n",
    "# Calculate compression ratios\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPRESSION COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "baseline = compression_results[\"none\"]\n",
    "print(f\"\\n{'Algorithm':<10} {'Size (bytes)':<15} {'Compression Ratio':<20} {'Reduction'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for algo, size in compression_results.items():\n",
    "    ratio = baseline / size if size > 0 else 0\n",
    "    reduction = (1 - size/baseline) * 100 if baseline > 0 else 0\n",
    "    print(f\"{algo:<10} {size:<15,} {ratio:<20.2f}x {reduction:>5.1f}% reduction\")\n",
    "\n",
    "print(f\"\\n\ud83d\udca1 Best compression: {min(compression_results, key=compression_results.get)}\")\n",
    "print(f\"\ud83d\udca1 Fastest (no compression): none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Verification\n",
    "\n",
    "Let's verify that everything worked correctly and summarize what we've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VERIFICATION CHECKLIST\n",
      "============================================================\n",
      "\u2713 Model loaded successfully\n",
      "\u2713 Prompt activations captured\n",
      "\u2713 Generation activations captured\n",
      "\u2713 MLP activations captured\n",
      "\u2717 Prompt analysis completed\n",
      "\u2713 Generation analysis completed\n",
      "\u2717 MLP analysis completed\n",
      "\u2713 Compression test completed\n",
      "\u2713 Model generated valid output\n",
      "\n",
      "============================================================\n",
      "\u2717 Some checks failed - please review the output above\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "SUMMARY: What You've Learned\n",
      "============================================================\n",
      "\n",
      "\u2713 How to configure the instrumentation framework with different settings\n",
      "\u2713 How to instrument a transformer model for activation capture\n",
      "\u2713 How to capture activations separately for:\n",
      "  - Prompt processing (input phase)\n",
      "  - Generation/reasoning (output phase)\n",
      "\u2713 How to use different granularities:\n",
      "  - FULL_TENSOR: Complete activation capture\n",
      "  - ATTENTION_ONLY: Focus on attention mechanisms\n",
      "  - MLP_ONLY: Focus on feedforward layers\n",
      "\u2713 How to compare compression algorithms:\n",
      "  - none: Fastest, largest files\n",
      "  - lz4: Fast compression, good ratio\n",
      "  - zstd: Best compression, slower\n",
      "\u2713 How to analyze captured activation streams\n",
      "\u2713 How to interpret per-layer statistics\n",
      "\n",
      "============================================================\n",
      "FILES GENERATED\n",
      "============================================================\n",
      "\n",
      "1. prompt_activations.stream (0 bytes)\n",
      "   \u2192 Attention-only activations from prompt processing\n",
      "\n",
      "2. generation_activations.stream (46,438,619 bytes)\n",
      "   \u2192 Full tensor activations from reasoning/generation\n",
      "\n",
      "3. mlp_activations.stream (0 bytes)\n",
      "   \u2192 MLP-only activations (uncompressed)\n",
      "\n",
      "4. compression_test_*.stream (3 files)\n",
      "   \u2192 Compression algorithm comparison\n",
      "\n",
      "\n",
      "============================================================\n",
      "NEXT STEPS\n",
      "============================================================\n",
      "\n",
      "Now you can:\n",
      "1. Use these activation streams for downstream analysis\n",
      "2. Feed them into interpretability tools (SAE, causal graphs)\n",
      "3. Correlate activations with model behavior\n",
      "4. Optimize model performance based on activation patterns\n",
      "5. Compare activations across different prompts/models\n",
      "\n",
      "For more information, see the documentation:\n",
      "- docs/API.md - Complete API reference\n",
      "- docs/STREAM_FORMAT.md - Stream format specification\n",
      "- docs/ARCHITECTURE.md - Framework architecture\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VERIFICATION CHECKLIST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "checks = [\n",
    "    (\"Model loaded successfully\", model is not None),\n",
    "    (\"Prompt activations captured\", os.path.exists(output_path_prompt)),\n",
    "    (\"Generation activations captured\", os.path.exists(output_path_generation)),\n",
    "    (\"Per-token activations captured\", os.path.exists(output_path_per_token)),\n",
    "    (\"MLP activations captured\", os.path.exists(output_path_mlp)),\n",
    "    (\"Prompt analysis completed\", analysis_prompt['packets'] > 0),\n",
    "    (\"Generation analysis completed\", analysis_generation['packets'] > 0),\n",
    "    (\"Per-token analysis completed\", 'token_metadata' in analysis_per_token),\n",
    "    (\"MLP analysis completed\", analysis_mlp['packets'] > 0),\n",
    "    (\"Compression test completed\", len(compression_results) == 3),\n",
    "    (\"Model generated valid output\", len(generated_text) > len(test_prompt)),\n",
    "]\n",
    "\n",
    "all_passed = True\n",
    "for check_name, result in checks:\n",
    "    status = \"\u2713\" if result else \"\u2717\"\n",
    "    print(f\"{status} {check_name}\")\n",
    "    if not result:\n",
    "        all_passed = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if all_passed:\n",
    "    print(\"\u2713 ALL CHECKS PASSED - Framework is working correctly!\")\n",
    "else:\n",
    "    print(\"\u2717 Some checks failed - please review the output above\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: What You've Learned\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "\u2713 How to configure the instrumentation framework with different settings\n",
    "\u2713 How to instrument a transformer model for activation capture\n",
    "\u2713 How to capture activations separately for:\n",
    "  - Prompt processing (input phase)\n",
    "  - Generation/reasoning (output phase)\n",
    "  - Per-token tracking for fine-grained analysis\n",
    "\u2713 How to use different granularities:\n",
    "  - FULL_TENSOR: Complete activation capture\n",
    "  - ATTENTION_ONLY: Focus on attention mechanisms\n",
    "  - MLP_ONLY: Focus on feedforward layers\n",
    "\u2713 How to compare compression algorithms:\n",
    "  - none: Fastest, largest files\n",
    "  - lz4: Fast compression, good ratio\n",
    "  - zstd: Best compression, slower\n",
    "\u2713 How to analyze captured activation streams\n",
    "\u2713 How to interpret per-layer statistics\n",
    "\u2713 How to track tokens during generation for reasoning analysis\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FILES GENERATED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "1. {output_path_prompt} ({os.path.getsize(output_path_prompt):,} bytes)\n",
    "   \u2192 Attention-only activations from prompt processing\n",
    "   \n",
    "2. {output_path_generation} ({os.path.getsize(output_path_generation):,} bytes)\n",
    "   \u2192 Full tensor activations from reasoning/generation\n",
    "   \n",
    "3. {output_path_mlp} ({os.path.getsize(output_path_mlp):,} bytes)\n",
    "   \u2192 MLP-only activations (uncompressed)\n",
    "   \n",
    "4. compression_test_*.stream (3 files)\n",
    "   \u2192 Compression algorithm comparison\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "Now you can:\n",
    "1. Use these activation streams for downstream analysis\n",
    "2. Feed them into interpretability tools (SAE, causal graphs)\n",
    "3. Correlate activations with model behavior\n",
    "4. Optimize model performance based on activation patterns\n",
    "5. Compare activations across different prompts/models\n",
    "\n",
    "For more information, see the documentation:\n",
    "- docs/API.md - Complete API reference\n",
    "- docs/STREAM_FORMAT.md - Stream format specification\n",
    "- docs/ARCHITECTURE.md - Framework architecture\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cleanup (Optional)\n",
    "\n",
    "Uncomment and run this cell to remove generated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# files_to_remove = [\n",
    "#     output_path_prompt,\n",
    "#     output_path_generation,\n",
    "        \"generation_per_token.stream\",\n",
    "        \"generation_per_token_tokens.json\",\n",
    "#     output_path_mlp,\n",
    "#     \"compression_test_none.stream\",\n",
    "#     \"compression_test_lz4.stream\",\n",
    "#     \"compression_test_zstd.stream\",\n",
    "# ]\n",
    "\n",
    "# for file_path in files_to_remove:\n",
    "#     if os.path.exists(file_path):\n",
    "#         os.remove(file_path)\n",
    "#         print(f\"Removed: {file_path}\")\n",
    "\n",
    "# print(\"\\n\u2713 Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}